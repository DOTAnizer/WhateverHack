{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "model_path = '/language-style-transfer/code'\n",
    "\n",
    "if model_path not in sys.path:\n",
    "    sys.path.append(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab import Vocabulary, build_vocab\n",
    "from accumulator import Accumulator\n",
    "from options import load_arguments\n",
    "from file_io import load_sent, write_sent\n",
    "from utils import *\n",
    "from nn import *\n",
    "import beam_search, greedy_decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from style_transfer import Model, transfer, create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "args = {\n",
    "    'batch_size': 32,\n",
    "    'beam': 1,\n",
    "    'dev': '/storage/data3/ods_dota_dev',\n",
    "    'dim_emb': 100,\n",
    "    'dim_y': 200,\n",
    "    'dim_z': 500,\n",
    "    'dropout_keep_prob': 0.5,\n",
    "    'embedding': '',\n",
    "    'filter_sizes': '1,2,3,4,5',\n",
    "    'gamma_decay': 1,\n",
    "    'gamma_init': 0.1,\n",
    "    'gamma_min': 0.1,\n",
    "    'learning_rate': 0.0005,\n",
    "    'load_model': True,\n",
    "    'max_epochs': 20,\n",
    "    'max_seq_length': 10,\n",
    "    'max_train_size': -1,\n",
    "    'model': '/storage/tmp/model',\n",
    "    'n_filters': 128,\n",
    "    'n_layers': 1,\n",
    "    'online_testing': False,\n",
    "    'output': '/storage/tmp/ods_dota.dev',\n",
    "    'rho': 1,\n",
    "    'steps_per_checkpoint': 1000,\n",
    "    'test': '',\n",
    "    'train': '/storage/data3/ods_dota',\n",
    "    'vocab': '/storage/tmp/ods_dota.vocab'\n",
    "}\n",
    "\n",
    "args = namedtuple('args', args.keys())(*args.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def remove_duplicates(tokens):\n",
    "    return [g[0] for g in groupby(tokens)]\n",
    "\n",
    "def decode_sentences(sentences):\n",
    "    return [' '.join(remove_duplicates(tokens)) for tokens in sentences]\n",
    "\n",
    "def transfer(model, decoder, sess, args, vocab, data0, data1):\n",
    "    batches, order0, order1 = get_batches(data0, data1,\n",
    "        vocab.word2id, args.batch_size)\n",
    "\n",
    "    #data0_rec, data1_rec = [], []\n",
    "    data0_tsf, data1_tsf = [], []\n",
    "    losses = Accumulator(len(batches), ['loss', 'rec', 'adv', 'd0', 'd1'])\n",
    "    for batch in batches:\n",
    "        rec, tsf = decoder.rewrite(batch)\n",
    "        half = batch['size'] / 2\n",
    "        #data0_rec += rec[:half]\n",
    "        #data1_rec += rec[half:]\n",
    "        data0_tsf += tsf[:half]\n",
    "        data1_tsf += tsf[half:]\n",
    "\n",
    "        loss, loss_rec, loss_adv, loss_d0, loss_d1 = sess.run([model.loss,\n",
    "            model.loss_rec, model.loss_adv, model.loss_d0, model.loss_d1],\n",
    "            feed_dict=feed_dictionary(model, batch, args.rho, args.gamma_min))\n",
    "        losses.add([loss, loss_rec, loss_adv, loss_d0, loss_d1])\n",
    "\n",
    "    n0, n1 = len(data0), len(data1)\n",
    "    #data0_rec = reorder(order0, data0_rec)[:n0]\n",
    "    #data1_rec = reorder(order1, data1_rec)[:n1]\n",
    "    data0_tsf = reorder(order0, data0_tsf)[:n0]\n",
    "    data1_tsf = reorder(order1, data1_tsf)[:n1]\n",
    "\n",
    "    return losses, data0_tsf, data1_tsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary(args.vocab, args.embedding, args.dim_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /storage/tmp/model\n",
      "INFO:tensorflow:Restoring parameters from /storage/tmp/model\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "model = create_model(sess, args, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.beam > 1:\n",
    "    decoder = beam_search.Decoder(sess, args, vocab, model)\n",
    "else:\n",
    "    decoder = greedy_decoding.Decoder(sess, args, vocab, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0 = [sentence.split(' ') for sentence in ['ты нахуя страты палишь буржуй .'] * args.batch_size]\n",
    "test1 = [sentence.split(' ') for sentence in ['ты нахуя страты палишь буржуй .'] * args.batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0 = list(np.random.choice(load_sent(args.dev + '.0'), args.batch_size))\n",
    "test1 = list(np.random.choice(load_sent(args.dev + '.1'), args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, tsf0, tsf1 = transfer(model, decoder, sess, args, vocab, test0, test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 21.05, rec 16.44, adv 4.61, d0 1.46, d1 1.77\n"
     ]
    }
   ],
   "source": [
    "losses.output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ODS -> DotA:\n",
      "всё в москве всё в _unk_ _\n",
      "всё в доте всё _unk_ в _ .\n",
      "\n",
      "у меня где-то лежит самый топовый рюкзак _unk_\n",
      "у меня мать самый такой топовый _unk_ .\n",
      "\n",
      "“ объясню тебе бустинг в очереди на _unk_ ”\n",
      "пп тебе повезло в лесу на аксе _unk_ сосать .\n",
      "\n",
      "моя карта по идее\n",
      "моя карта по идее как бы медом .\n",
      "\n",
      "seriously though смотри urban dictionary\n",
      "<unk> смотри блять ) изи мид .\n",
      "\n",
      "никто из топа не улучшил скор за последние три дня\n",
      "никто из хуй не нравится 2 дня 2 дня .\n",
      "\n",
      "но там тогда самый новый элемент не получить\n",
      "давай там новый очко не успел получить .\n",
      "\n",
      "для галочки посмотрел можно _unk_ пропускать\n",
      "мать на рики _unk_ можно армлет ебаный .\n",
      "\n",
      "это js фреймворки меняются\n",
      "это закуп ебем или пуле мы тоже .\n",
      "\n",
      "сгореть ага . прямо как _unk_ пятнадцать лет назад\n",
      ") как прямо _unk_ лайк лет назад .\n",
      "\n",
      "взял бы да перевёл на _unk_ основе\n",
      "да взял бы на _unk_ репорт днище .\n",
      "\n",
      "дача для быдла\n",
      "лохи для меня ? .\n",
      "\n",
      "туплю под вечер\n",
      "ебать под тавер ) серьёзно . ) .\n",
      "\n",
      "это винда небось\n",
      "это там небось ? тоже началось .\n",
      "\n",
      "сортирую по папкам тэги не _unk_\n",
      "<unk> по _unk_ не роумер в эх .\n",
      "\n",
      "это поэт и из\n",
      "это и из _unk_ ? я тоже .\n",
      "\n",
      "побить надо на паблике или на привате ?\n",
      "заебал на лечение мне так на ? .\n",
      "\n",
      "купи _unk_\n",
      "купи ) . _unk_ .\n",
      "\n",
      "кстати и apple\n",
      "кстати и цм 0 . так ? .\n",
      "\n",
      "а зачем картинки клеить ?\n",
      "а чо ? .\n",
      "\n",
      "они исправили расчет награды и _unk_ все очки\n",
      "они _unk_ и все ворвался и все .\n",
      "\n",
      "ничего плохого в этом нет . но для них )\n",
      "ничего в вас играю ) а для нет .\n",
      "\n",
      "я тоже хочу на _unk_ сходить марта _slightly_smiling_face_\n",
      "я тоже хочу на _unk_ ? блеать .\n",
      "\n",
      "можно но мне футболки хватит\n",
      "репорт это сука хватит заебали суки .\n",
      "\n",
      "спасибо !\n",
      ") ! .\n",
      "\n",
      "по второму вопросу есть интересная новость\n",
      "по твоему годом ! хороший ебанутая новость .\n",
      "\n",
      "вот русских учёных _unk_ а занимаются тем же самым\n",
      "вот _unk_ хуесосы ну же катки тем лучше .\n",
      "\n",
      "только опять же как туда впихнуть cnn ?\n",
      "опять же мы туда ? педик ням .\n",
      "\n",
      "вот dropbox такое пообещал а oculus нет\n",
      "вот такое а потому что ты пзд нет .\n",
      "\n",
      "но запись же будет да ?\n",
      "гг пизда будет но да ? изи .\n",
      "\n",
      "моя знать только цпт и _unk_\n",
      "моя раз могу и _unk_ и забрал .\n",
      "\n",
      "пруф .\n",
      "пруф . ! .\n",
      "\n",
      "DotA -> ODS:\n",
      "? .\n",
      "<unk> то\n",
      "\n",
      "с таким од хуй что сделаешь од _unk_ .\n",
      "с ним знает что бы _unk_ кажется\n",
      "\n",
      "спасибо сф )  я тебя люблю ? купол .\n",
      "спасибо я и ? ) люблю тебя % купол ?\n",
      "\n",
      "рофл орб _unk_ 01 сек не смог .\n",
      "рофл _unk_ не меньше медленнее\n",
      "\n",
      "с новым годом епте я _unk_ .\n",
      "с <unk> _unk_ <unk> я _unk_ чай\n",
      "\n",
      "мать ебал как вы сука увидили мрази ебаные .\n",
      "мать как эти мрази слоя используются\n",
      "\n",
      "я рот твой ебал пизда тебе .\n",
      "я твой сейчас тебе мать сейчас _simple_smile_\n",
      "\n",
      "красиво гг что мог то и делал .\n",
      "( что мог и то делал\n",
      "\n",
      "ебать ты даун братишка ) сосать .\n",
      "ебать ты бы будешь лежать выше )\n",
      "\n",
      "чет изи и что ? ага .\n",
      "чет и что ? ага это (\n",
      "\n",
      "пиздец чен даун репорт плиз _unk_ сасат .\n",
      "_unk_ можно плиз можно трансляции впрочем\n",
      "\n",
      "красиво гг что мог то и делал .\n",
      "( что мог и то делал\n",
      "\n",
      ".\n",
      ". <unk> )\n",
      "\n",
      "чё тянуть ? . ! .\n",
      "чё тогда ? . ! . <unk> . ! .\n",
      "\n",
      "троль лс но не уебок .\n",
      "видимо мусор не знает а там есть _unk_\n",
      "\n",
      "ебнутые ( вы бы хоть _unk_ .\n",
      "ну хоть бы _unk_ больше еще он посмотрим\n",
      "\n",
      "пизда тебе мепо ахаха ливнул ска блять сек 1 .\n",
      "тебе же ) сложнее блять 1 раз месяцев\n",
      "\n",
      "цыган коней своих забери в свой _unk_ .\n",
      "бы себя _unk_ в свой группу\n",
      "\n",
      "баш мать в _unk_ урод ты мать продал .\n",
      "сейчас в _unk_ урод есть продал\n",
      "\n",
      "ахах 4 руны вы тупые ? .\n",
      "ахах вы три нужны это они используют _nabros_\n",
      "\n",
      "у вас рандом пацаны ? ) .\n",
      "у вас пацаны\n",
      "\n",
      "вы шо мне руна нужна хахаха .\n",
      "это кому\n",
      "\n",
      "боже этот джагернаут просто худший ) .\n",
      "боже этот просто _unk_ там <unk> ряды нет\n",
      "\n",
      "лол ? ты охуел ? сларк всё правильно делает .\n",
      "лол ты охуел ? делает всё правильно делает\n",
      "\n",
      "иди нахуй _unk_ ебало те сломаю .\n",
      "иди _unk_ опять прям сейчас гляну сломаю\n",
      "\n",
      "иди нахуй даун ! .\n",
      "иди пошел ! будет !\n",
      "\n",
      "1 2 3 4 5 ахаха трооон .\n",
      "deep <unk> или <unk> изображений\n",
      "\n",
      "о боже я даун прикольно ) .\n",
      "о я там вопрос\n",
      "\n",
      "это тупо слив керри не фармить пришел на мид .\n",
      "это тупо <unk> не хочу на бот пришел\n",
      "\n",
      "плиз репорт пуджа уебана конченый руинер .\n",
      "можно нельзя несколько <unk> посмотреть она прикрутить\n",
      "\n",
      "изи _unk_ нули бля просто на изи .\n",
      "но _unk_ просто на маке просто\n",
      "\n",
      "лох ? .\n",
      "лох ? <unk> так\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('ODS -> DotA:')\n",
    "for s1, s2 in zip(decode_sentences(test0), decode_sentences(tsf0)):\n",
    "    print(\"%s\\n%s\\n\" % (s1, s2))\n",
    "\n",
    "print('DotA -> ODS:')\n",
    "for s1, s2 in zip(decode_sentences(test1), decode_sentences(tsf1)):\n",
    "    print(\"%s\\n%s\\n\" % (s1, s2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
